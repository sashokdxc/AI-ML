{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dc669d4d-2da0-44e4-ba35-199cc41175d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 60000 images.\n",
      "Size of training set: 10000 images.\n",
      "Image dimensions: torch.Size([1, 28, 28]).\n",
      "Image label: 5.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAa1klEQVR4nO3dfWyV9f3/8dcp0ANqe1gp7Wm5LaBiQFjGoHYqk9H1ZosTZIsoWWBxGlgxA+ZNuiioc+m+LNmMS8H9M5iZeJcMGMSQYLUlaoGAEGbUjjZ1lNEWJeOcUqAQ+vn9wc8zjrTAdTin757T5yP5JPSc69Pz3rUTnl49pwefc84JAIA+lmY9AABgYCJAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADAxGDrAb6uu7tbx44dU0ZGhnw+n/U4AACPnHPq6OhQfn6+0tJ6v87pdwE6duyYxowZYz0GAOA6tbS0aPTo0b3e3+9+BJeRkWE9AgAgDq7293nCAlRdXa3x48dr6NChKiws1N69e69pHz92A4DUcLW/zxMSoDfeeEOrVq3SmjVr9NFHH2n69OkqLS3V8ePHE/FwAIBk5BJg1qxZrqKiIvL1hQsXXH5+vquqqrrq3lAo5CSxWCwWK8lXKBS64t/3cb8COnfunPbv36/i4uLIbWlpaSouLlZ9ff1lx3d1dSkcDkctAEDqi3uAvvzyS124cEG5ublRt+fm5qqtre2y46uqqhQIBCKLd8ABwMBg/i64yspKhUKhyGppabEeCQDQB+L+e0DZ2dkaNGiQ2tvbo25vb29XMBi87Hi/3y+/3x/vMQAA/Vzcr4DS09M1Y8YM1dTURG7r7u5WTU2NioqK4v1wAIAklZBPQli1apUWL16sb3/725o1a5ZefPFFdXZ26mc/+1kiHg4AkIQSEqAHHnhAX3zxhVavXq22tjZ985vf1I4dOy57YwIAYODyOeec9RCXCofDCgQC1mMAAK5TKBRSZmZmr/ebvwsOADAwESAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYGWw8A9Cff//73Pe/56U9/6nnPokWLPO85ePCg5z3f+973PO+RpFAoFNM+wAuugAAAJggQAMBE3AP07LPPyufzRa3JkyfH+2EAAEkuIa8BTZkyRe+8887/HmQwLzUBAKIlpAyDBw9WMBhMxLcGAKSIhLwGdPjwYeXn52vChAlatGiRjhw50uuxXV1dCofDUQsAkPriHqDCwkJt3LhRO3bs0Pr169Xc3Ky7775bHR0dPR5fVVWlQCAQWWPGjIn3SACAfijuASovL9dPfvITTZs2TaWlpXr77bd18uRJvfnmmz0eX1lZqVAoFFktLS3xHgkA0A8l/N0Bw4cP1y233KLGxsYe7/f7/fL7/YkeAwDQzyT894BOnTqlpqYm5eXlJfqhAABJJO4Bevzxx1VXV6fPP/9cH374oebPn69BgwbpwQcfjPdDAQCSWNx/BHf06FE9+OCDOnHihEaOHKm77rpLu3fv1siRI+P9UACAJOZzzjnrIS4VDocVCASsx8AA9eGHH3reM2vWrARMEh+ffPJJTPt+/vOfe96zd+/emB4LqSsUCikzM7PX+/ksOACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADARML/QTrAQnl5eUz7ZsyYEedJbE2ZMiWmfStXrvS8h39yBV5xBQQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATfBo2UlJhYWFM+3w+X5wnSU4jRoywHgEDAFdAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJn3POWQ9xqXA4rEAgYD0GBqjW1lbPe0aOHJmASeIj1g9Xramp8bynpKQkpsdC6gqFQsrMzOz1fq6AAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATg60HAJLd3r17Pe8ZP3685z05OTme9wD9GVdAAAATBAgAYMJzgHbt2qV7771X+fn58vl82rJlS9T9zjmtXr1aeXl5GjZsmIqLi3X48OF4zQsASBGeA9TZ2anp06erurq6x/vXrl2rl156SS+//LL27NmjG2+8UaWlpTp79ux1DwsASB2e34RQXl6u8vLyHu9zzunFF1/U008/rfvuu0+S9Morryg3N1dbtmzRwoULr29aAEDKiOtrQM3NzWpra1NxcXHktkAgoMLCQtXX1/e4p6urS+FwOGoBAFJfXAPU1tYmScrNzY26PTc3N3Lf11VVVSkQCETWmDFj4jkSAKCfMn8XXGVlpUKhUGS1tLRYjwQA6ANxDVAwGJQktbe3R93e3t4eue/r/H6/MjMzoxYAIPXFNUAFBQUKBoOqqamJ3BYOh7Vnzx4VFRXF86EAAEnO87vgTp06pcbGxsjXzc3NOnjwoLKysjR27FitWLFCL7zwgm6++WYVFBTomWeeUX5+vubNmxfPuQEASc5zgPbt26c5c+ZEvl61apUkafHixdq4caOefPJJdXZ26tFHH9XJkyd11113aceOHRo6dGj8pgYAJD2fc85ZD3GpcDisQCBgPQYGqEt/heBaffDBB573/OMf//C859L/8LtWPp/P8x5Jampq8rxnw4YNMT1Wf7Zu3TrPe0KhUAImSU6hUOiKr+ubvwsOADAwESAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwASfhg1cp1j+qZH33nvP856ZM2d63hPrp2H3s78WzOzdu9fznu985zsJmCQ58WnYAIB+iQABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwMdh6ACDZlZSUeN4TyweLou9NmTLFeoSUxhUQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCDyMFLjFr1izPe5566qkETIJ4+te//hXTvueffz7Ok+BSXAEBAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACb4MFKkpFGjRsW077e//a3nPXfccUdMj9UX0tJi+2/MvXv3et6zadMmz3vWrVvneU9xcbHnPc45z3skaceOHTHtw7XhCggAYIIAAQBMeA7Qrl27dO+99yo/P18+n09btmyJun/JkiXy+XxRq6ysLF7zAgBShOcAdXZ2avr06aquru71mLKyMrW2tkbWa6+9dl1DAgBSj+c3IZSXl6u8vPyKx/j9fgWDwZiHAgCkvoS8BlRbW6ucnBzdeuutWrZsmU6cONHrsV1dXQqHw1ELAJD64h6gsrIyvfLKK6qpqdH//d//qa6uTuXl5bpw4UKPx1dVVSkQCETWmDFj4j0SAKAfivvvAS1cuDDy59tvv13Tpk3TxIkTVVtbq7lz5152fGVlpVatWhX5OhwOEyEAGAAS/jbsCRMmKDs7W42NjT3e7/f7lZmZGbUAAKkv4QE6evSoTpw4oby8vEQ/FAAgiXj+EdypU6eirmaam5t18OBBZWVlKSsrS88995wWLFigYDCopqYmPfnkk5o0aZJKS0vjOjgAILl5DtC+ffs0Z86cyNdfvX6zePFirV+/XocOHdJf//pXnTx5Uvn5+SopKdFvfvMb+f3++E0NAEh6Phfrp/QlSDgcViAQsB4D/Ugsz4f3338/pse67bbbYtrXX+3fvz+mfT/60Y8872lvb4/psZC6QqHQFV/X57PgAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYCLu/yQ3cCXDhg3zvGfTpk2e96Tap1rHKpZzJ/HJ1ugbXAEBAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACZ8zjlnPcSlwuGwAoGA9RhIkPHjx3ve09jYGP9BktCZM2c878nKyorpsc6fPx/TPuBSoVBImZmZvd7PFRAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYGKw9QAYWJ5++mnrEZLWCy+84HkPHyqK/owrIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABB9GiphNnDjR856HHnooAZMMDG+88Yb1CEBccQUEADBBgAAAJjwFqKqqSjNnzlRGRoZycnI0b948NTQ0RB1z9uxZVVRUaMSIEbrpppu0YMECtbe3x3VoAEDy8xSguro6VVRUaPfu3dq5c6fOnz+vkpISdXZ2Ro5ZuXKltm3bprfeekt1dXU6duyY7r///rgPDgBIbj7nnIt18xdffKGcnBzV1dVp9uzZCoVCGjlypDZt2qQf//jHkqTPPvtMt912m+rr63XHHXdc9XuGw2EFAoFYR0IfiuVNCP/85z8970lPT/e8JxVNmjTJ857PP/88/oMA1ygUCikzM7PX+6/rNaBQKCRJysrKkiTt379f58+fV3FxceSYyZMna+zYsaqvr+/xe3R1dSkcDkctAEDqizlA3d3dWrFihe68805NnTpVktTW1qb09HQNHz486tjc3Fy1tbX1+H2qqqoUCAQia8yYMbGOBABIIjEHqKKiQh9//LFef/316xqgsrJSoVAoslpaWq7r+wEAkkNMv4i6fPlybd++Xbt27dLo0aMjtweDQZ07d04nT56Mugpqb29XMBjs8Xv5/X75/f5YxgAAJDFPV0DOOS1fvlybN2/Wu+++q4KCgqj7Z8yYoSFDhqimpiZyW0NDg44cOaKioqL4TAwASAmeroAqKiq0adMmbd26VRkZGZHXdQKBgIYNG6ZAIKCHH35Yq1atUlZWljIzM/XYY4+pqKjomt4BBwAYODwFaP369ZKke+65J+r2DRs2aMmSJZKkP/7xj0pLS9OCBQvU1dWl0tJSrVu3Li7DAgBSh6cAXcuvDA0dOlTV1dWqrq6OeSgkhzlz5njew+/0XPTJJ5943vPVrz0AqYLPggMAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAICJmP5FVAD/85///MfznrKyMs97/vvf/3reA/RnXAEBAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACb4MFLE7O233/a859NPP/W857bbbvO8py/95S9/8bzn2LFjCZgESC5cAQEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJvgwUsQslg/UnD9/vuc9n332mec9b775puc9krRt2zbPe7Zv3x7TYwEDHVdAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJPowUfaqxsdHznsGDeZoCqYgrIACACQIEADDhKUBVVVWaOXOmMjIylJOTo3nz5qmhoSHqmHvuuUc+ny9qLV26NK5DAwCSn6cA1dXVqaKiQrt379bOnTt1/vx5lZSUqLOzM+q4Rx55RK2trZG1du3auA4NAEh+nl7d3bFjR9TXGzduVE5Ojvbv36/Zs2dHbr/hhhsUDAbjMyEAICVd12tAoVBIkpSVlRV1+6uvvqrs7GxNnTpVlZWVOn36dK/fo6urS+FwOGoBAAYAF6MLFy64H/7wh+7OO++Muv3Pf/6z27Fjhzt06JD729/+5kaNGuXmz5/f6/dZs2aNk8RisVisFFuhUOiKHYk5QEuXLnXjxo1zLS0tVzyupqbGSXKNjY093n/27FkXCoUiq6WlxfyksVgsFuv619UCFNNv+C1fvlzbt2/Xrl27NHr06CseW1hYKOniLyBOnDjxsvv9fr/8fn8sYwAAkpinADnn9Nhjj2nz5s2qra1VQUHBVfccPHhQkpSXlxfTgACA1OQpQBUVFdq0aZO2bt2qjIwMtbW1SZICgYCGDRumpqYmbdq0ST/4wQ80YsQIHTp0SCtXrtTs2bM1bdq0hPwPAAAkKS+v+6iXn/Nt2LDBOefckSNH3OzZs11WVpbz+/1u0qRJ7oknnrjqzwEvFQqFzH9uyWKxWKzrX1f7u9/3/8PSb4TDYQUCAesxAADXKRQKKTMzs9f7+Sw4AIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAICJfhcg55z1CACAOLja3+f9LkAdHR3WIwAA4uBqf5/7XD+75Oju7taxY8eUkZEhn88XdV84HNaYMWPU0tKizMxMowntcR4u4jxcxHm4iPNwUX84D845dXR0KD8/X2lpvV/nDO7Dma5JWlqaRo8efcVjMjMzB/QT7Cuch4s4DxdxHi7iPFxkfR4CgcBVj+l3P4IDAAwMBAgAYCKpAuT3+7VmzRr5/X7rUUxxHi7iPFzEebiI83BRMp2HfvcmBADAwJBUV0AAgNRBgAAAJggQAMAEAQIAmEiaAFVXV2v8+PEaOnSoCgsLtXfvXuuR+tyzzz4rn88XtSZPnmw9VsLt2rVL9957r/Lz8+Xz+bRly5ao+51zWr16tfLy8jRs2DAVFxfr8OHDNsMm0NXOw5IlSy57fpSVldkMmyBVVVWaOXOmMjIylJOTo3nz5qmhoSHqmLNnz6qiokIjRozQTTfdpAULFqi9vd1o4sS4lvNwzz33XPZ8WLp0qdHEPUuKAL3xxhtatWqV1qxZo48++kjTp09XaWmpjh8/bj1an5syZYpaW1sj6/3337ceKeE6Ozs1ffp0VVdX93j/2rVr9dJLL+nll1/Wnj17dOONN6q0tFRnz57t40kT62rnQZLKysqinh+vvfZaH06YeHV1daqoqNDu3bu1c+dOnT9/XiUlJers7Iwcs3LlSm3btk1vvfWW6urqdOzYMd1///2GU8fftZwHSXrkkUeing9r1641mrgXLgnMmjXLVVRURL6+cOGCy8/Pd1VVVYZT9b01a9a46dOnW49hSpLbvHlz5Ovu7m4XDAbd73//+8htJ0+edH6/37322msGE/aNr58H55xbvHixu++++0zmsXL8+HEnydXV1TnnLv5/P2TIEPfWW29Fjvn000+dJFdfX281ZsJ9/Tw459x3v/td98tf/tJuqGvQ76+Azp07p/3796u4uDhyW1pamoqLi1VfX284mY3Dhw8rPz9fEyZM0KJFi3TkyBHrkUw1Nzerra0t6vkRCARUWFg4IJ8ftbW1ysnJ0a233qply5bpxIkT1iMlVCgUkiRlZWVJkvbv36/z589HPR8mT56ssWPHpvTz4evn4SuvvvqqsrOzNXXqVFVWVur06dMW4/Wq330Y6dd9+eWXunDhgnJzc6Nuz83N1WeffWY0lY3CwkJt3LhRt956q1pbW/Xcc8/p7rvv1scff6yMjAzr8Uy0tbVJUo/Pj6/uGyjKysp0//33q6CgQE1NTfr1r3+t8vJy1dfXa9CgQdbjxV13d7dWrFihO++8U1OnTpV08fmQnp6u4cOHRx2bys+Hns6DJD300EMaN26c8vPzdejQIT311FNqaGjQ3//+d8Npo/X7AOF/ysvLI3+eNm2aCgsLNW7cOL355pt6+OGHDSdDf7Bw4cLIn2+//XZNmzZNEydOVG1trebOnWs4WWJUVFTo448/HhCvg15Jb+fh0Ucfjfz59ttvV15enubOnaumpiZNnDixr8fsUb//EVx2drYGDRp02btY2tvbFQwGjabqH4YPH65bbrlFjY2N1qOY+eo5wPPjchMmTFB2dnZKPj+WL1+u7du367333ov651uCwaDOnTunkydPRh2fqs+H3s5DTwoLCyWpXz0f+n2A0tPTNWPGDNXU1ERu6+7uVk1NjYqKigwns3fq1Ck1NTUpLy/PehQzBQUFCgaDUc+PcDisPXv2DPjnx9GjR3XixImUen4457R8+XJt3rxZ7777rgoKCqLunzFjhoYMGRL1fGhoaNCRI0dS6vlwtfPQk4MHD0pS/3o+WL8L4lq8/vrrzu/3u40bN7pPPvnEPfroo2748OGura3NerQ+9atf/crV1ta65uZm98EHH7ji4mKXnZ3tjh8/bj1aQnV0dLgDBw64AwcOOEnuD3/4gztw4ID797//7Zxz7ne/+50bPny427p1qzt06JC77777XEFBgTtz5ozx5PF1pfPQ0dHhHn/8cVdfX++am5vdO++84771rW+5m2++2Z09e9Z69LhZtmyZCwQCrra21rW2tkbW6dOnI8csXbrUjR071r377rtu3759rqioyBUVFRlOHX9XOw+NjY3u+eefd/v27XPNzc1u69atbsKECW727NnGk0dLigA559yf/vQnN3bsWJeenu5mzZrldu/ebT1Sn3vggQdcXl6eS09Pd6NGjXIPPPCAa2xstB4r4d577z0n6bK1ePFi59zFt2I/88wzLjc31/n9fjd37lzX0NBgO3QCXOk8nD592pWUlLiRI0e6IUOGuHHjxrlHHnkk5f4jraf//ZLchg0bIsecOXPG/eIXv3Df+MY33A033ODmz5/vWltb7YZOgKudhyNHjrjZs2e7rKws5/f73aRJk9wTTzzhQqGQ7eBfwz/HAAAw0e9fAwIApCYCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwMT/A3D9gdDR0PADAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "training_ds = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(), # A quick way to convert the image from PIL image to tensor\n",
    ")\n",
    "\n",
    "test_ds = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "class LocalDS(Dataset):\n",
    "    def __init__(self, data_dir, label_dir, root_dir, transforms):\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        #Creating path lists\n",
    "        self.img_paths = os.path.join(self.root_dir, self.data_dir)\n",
    "        self.label_paths = os.path.join(self.root_dir, self.label_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        \"\"\"\n",
    "            This is the critical method in this class. It allows us to get \n",
    "            an instance by an id.\n",
    "        \"\"\"\n",
    "        img = Image.open(self.img_paths[idx])\n",
    "        label = label_paths[idx]\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, label \n",
    "\n",
    "print(f\"Size of training set: {len(training_ds)} images.\")\n",
    "print(f\"Size of training set: {len(test_ds)} images.\")\n",
    "\n",
    "img, lbl = training_ds[0]\n",
    "print(f\"Image dimensions: {img.shape}.\")\n",
    "print(f\"Image label: {lbl}.\")\n",
    "\n",
    "random_id = random.randint(0, len(training_ds))\n",
    "img, lbl = training_ds[random_id]\n",
    "\n",
    "img.squeeze().shape\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "\n",
    "lbl\n",
    "\n",
    "train_dataloader = DataLoader(training_ds, batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=128, shuffle=True)\n",
    "\n",
    "# PyTorch NN Building\n",
    "\n",
    "class FFNN(nn.Module): # Extending nn.Module allows us to create NNs\n",
    "    def __init__(self):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)  # Input is 28*28 = 784, output is 128 (can be anything)\n",
    "        self.fc2 = nn.Linear(128, 512)    # Input is 128 since output of fc1 is 128 \n",
    "        self.fc3 = nn.Linear(512, 128)    # Input is 512 since output of fc2 is 512\n",
    "        self.fc4 = nn.Linear(128, 10)     # Input is 128 since output of fc3 is 128, \n",
    "                                          #   and output is 10 since there are 10 classes \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)         # Flattening\n",
    "        x = F.relu(self.fc1(x))           # We feed the flattened images to to fc1 and perform ReLU   \n",
    "        x = F.relu(self.fc2(x))           # We do the same for all FC layers\n",
    "        x = F.relu(self.fc3(x))\n",
    "        logits = self.fc4(x)              # Finally, we get the predictions from fc4 or the output layer\n",
    "        return logits\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "model = FFNN()\n",
    "model = model.to(device)\n",
    "model \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "\n",
    "#Training\n",
    "\n",
    "def run_epoch(ep_id, action, loader, model, optimizer, criterion):\n",
    "    accuracies = [] # Keep list of accuracies to track progress\n",
    "    is_training = action == \"train\" # True when action == \"train\", else False \n",
    "\n",
    "    # Looping over all batches\n",
    "    for batch_idx, batch in enumerate(loader): \n",
    "            imgs, lbls = batch\n",
    "\n",
    "            # Sending images and labels to device \n",
    "            imgs = imgs.to(device)\n",
    "            lbls = lbls.to(device)\n",
    "\n",
    "            # Resetting the optimizer gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Setting model to train or test\n",
    "            with torch.set_grad_enabled(is_training):\n",
    "                \n",
    "                # Feed batch to model\n",
    "                logits = model(imgs)\n",
    "\n",
    "                # Calculate the loss based on predictions and real labels\n",
    "                loss = criterion(logits, lbls)\n",
    "\n",
    "                # Using torch.max() to get the highest prediction\n",
    "                _, preds = torch.max(logits, 1)\n",
    "\n",
    "                # Calculating accuracy between real labels and predicted labels\n",
    "                # Notice that tensors must be on CPU to perform such calculations\n",
    "                acc = accuracy_score(preds.to('cpu'), lbls.to('cpu'))\n",
    "\n",
    "                # If training, perform backprop and update weights\n",
    "                if is_training:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # Append current batch accuracy\n",
    "                accuracies.append(acc)\n",
    "\n",
    "                # Print some stats every 50th batch \n",
    "                if batch_idx % 50 == 0:\n",
    "                    print(f\"{action.capitalize()}ing, Epoch: {ep_id+1}, Batch {batch_idx}: Loss = {loss.item()}, Acc = {acc}\")\n",
    "    # Return accuracies to main loop                 \n",
    "    return accuracies\n",
    "\n",
    "def main(epochs, train_dl, test_dl, model, optimizer, criterion):\n",
    "\n",
    "    # Keep lists of accuracies to track performance on train and test sets\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    # Looping over epochs\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Looping over train set and training\n",
    "        train_acc = run_epoch(epoch, \"train\", train_dl, model, optimizer, criterion)\n",
    "\n",
    "        # Looping over test set\n",
    "        test_acc = run_epoch(epoch, \"test\", test_dl, model, optimizer, criterion) \n",
    "\n",
    "        # Collecting stats\n",
    "        train_accuracies += train_acc\n",
    "        test_accuracies += test_acc         \n",
    "            \n",
    "    return train_accuracies, test_accuracies\n",
    "    train_accs, test_accs = main(3, train_dataloader, test_dataloader, model, optim, criterion)\n",
    "\n",
    "#plt.plot(train_accs)\n",
    "\n",
    "#plt.plot(test_accs)\n",
    "\n",
    "# Get a random test image\n",
    "random_id = random.randint(0, len(test_ds))\n",
    "img, lbl = training_ds[random_id]\n",
    "\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "\n",
    "lbl\n",
    "\n",
    "# First, send the image to device\n",
    "img = img.to(device)\n",
    "\n",
    "# Feed the image to the model\n",
    "logits = model(img)\n",
    "\n",
    "# Get the class with the highest score\n",
    "_, preds = torch.max(logits, 1)\n",
    "pred = preds.item()\n",
    "pred\n",
    "\n",
    "pred == lbl\n",
    "\n",
    "\n",
    "#Saving/Loading Models\n",
    "\n",
    "# Saving current weights:\n",
    "path = \"mnist_model.pt\"\n",
    "torch.save(model.state_dict(), path)\n",
    "new_model = FFNN()\n",
    "new_model = new_model.to(device)\n",
    "\n",
    "logits = new_model(img)\n",
    "_, preds = torch.max(logits, 1)\n",
    "pred = preds.item()\n",
    "pred \n",
    "pred == lbl\n",
    "\n",
    "new_model.load_state_dict(torch.load(path))\n",
    "\n",
    "logits = new_model(img)\n",
    "_, preds = torch.max(logits, 1)\n",
    "pred = preds.item()\n",
    "pred \n",
    "\n",
    "pred == lbl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702dbdde-603a-471d-9a3d-26724e61229d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
